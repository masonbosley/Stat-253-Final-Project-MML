tidy(final_fit_se)
glmnet_output <- final_fit_se %>% extract_fit_parsnip() %>% pluck('fit') # get the original glmnet output
lambdas <- glmnet_output$lambda
coefs_lambdas <-
coefficients(glmnet_output, s = lambdas )  %>%
as.matrix() %>%
t() %>%
as.data.frame() %>%
mutate(lambda = lambdas ) %>%
select(lambda, everything(), -`(Intercept)`) %>%
pivot_longer(cols = -lambda,
names_to = "term",
values_to = "coef") %>%
mutate(var = purrr::map_chr(stringr::str_split(term,"_"),~.[1]))
coefs_lambdas %>%
ggplot(aes(x = lambda, y = coef, group = term, color = var)) +
geom_line() +
geom_vline(xintercept = best_se_penalty %>% pull(penalty), linetype = 'dashed') +
theme_classic() +
theme(legend.position = "bottom", legend.text=element_text(size=8)) +
xlim(0,7700000) + ylim(-2500000,5000000)
nba2021per36 <- read.csv("Per36-Player.csv")
nba2021_2022salaries <- read.csv("2021-2022-Player-Salaries.csv")
names(nba2021_2022salaries)[1] <- "Player"
nba2021per36 <- nba2021per36 %>% left_join(nba2021_2022salaries,by=c('Player')) %>%
select(-AllStar)  %>%
distinct(Player, .keep_all = TRUE) %>%
filter(!is.na(Salary)) %>%
filter(MP > 300) %>%
na.omit()
head(nba2021per36)
tree_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = tune(),
min_n = 2,
tree_depth = NULL) %>%
set_mode('classification')
nba2021per36 <- nba2021per36 %>%
mutate(Pos = stringr::str_replace(stringr::str_sub(Pos,1,2),"-",""))
nba2021per36 <- nba2021per36 %>%
mutate(Pos = as.factor(Pos))
data_fold <- vfold_cv(nba2021per36, v = 10)
data_rec <- recipe(Pos ~ FG +FGA + FT. + PTS +STL + BLK + ORB + DRB + AST + X3P. + X2P. + TOV, data = nba2021per36)
data_wf_tune <- workflow() %>%
add_model(tree_spec) %>%
add_recipe(data_rec)
param_grid <- grid_regular(cost_complexity(range = c(-5, -1)), levels = 10)
tune_res <- tune_grid(
data_wf_tune,
resamples = data_fold,
grid = param_grid,
metrics = metric_set(accuracy) #change this for regression trees
)
autoplot(tune_res) + theme_classic()
best_complexity <- select_by_one_std_err(tune_res, metric = 'accuracy', desc(cost_complexity))
best_complexity %>%
pull(cost_complexity)
data_wf_final <- finalize_workflow(data_wf_tune, best_complexity)
pos_final_fit <- fit(data_wf_final, data = nba2021per36)
tune_res %>%
collect_metrics() %>%
filter(cost_complexity == best_complexity %>% pull(cost_complexity))
tree_mod_lowcp <- fit(
data_wf_tune %>%
update_model(tree_spec %>% set_args(cost_complexity = .012)),
data = nba2021per36
)
tree_mod_highcp <- fit(
data_wf_tune %>%
update_model(tree_spec %>% set_args(cost_complexity = .018)),
data = nba2021per36
)
# Plot all 3 trees in a row
#par(mfrow = c(1,3))
tree_mod_lowcp %>% extract_fit_engine() %>% rpart.plot()
pos_final_fit %>% extract_fit_engine() %>% rpart.plot()
tree_mod_highcp %>% extract_fit_engine() %>% rpart.plot()
library(vip)
conflicted::conflict_prefer("vi", "vip")
rf_spec <- rand_forest() %>%
set_engine(engine = 'ranger') %>%
set_args(mtry = NULL, # size of random subset of variables; default is floor(sqrt(number of total predictors))
trees = 1000, # Number of trees
min_n = 2,
probability = FALSE, # FALSE: get hard predictions (not needed for regression)
importance = 'impurity') %>% # we'll come back to this at the end
set_mode('classification') # change this for regression
# Recipe
data_rec <- recipe(Pos ~ FG +FGA + PTS +STL + BLK + Age + TOV + PF + ORB + DRB + TRB + AST + X3P + X3PA + X2P + X2PA + FT + FTA + FT., data = nba2021per36)
# Workflows
data_wf_mtry2 <- workflow() %>%
add_model(rf_spec %>% set_args(mtry = 2)) %>%
add_recipe(data_rec)
data_wf_mtry4 <- workflow() %>%
add_model(rf_spec %>% set_args(mtry = 4)) %>%
add_recipe(data_rec)
data_wf_mtry9 <- workflow() %>%
add_model(rf_spec %>% set_args(mtry = 9)) %>%
add_recipe(data_rec)
data_wf_mtry19 <- workflow() %>%
add_model(rf_spec %>% set_args(mtry = 19)) %>%
add_recipe(data_rec)
# Fit Models
set.seed(123)
data_fit_mtry2 <- fit(data_wf_mtry2, data = nba2021per36)
set.seed(123)
data_fit_mtry4 <- fit(data_wf_mtry4, data = nba2021per36)
set.seed(123)
data_fit_mtry9 <- fit(data_wf_mtry9, data = nba2021per36)
set.seed(123)
data_fit_mtry19 <- fit(data_wf_mtry19, data = nba2021per36)
# Evaluate OOB Metrics
data_rf_OOB_output <- bind_rows(
rf_OOB_output(data_fit_mtry2,2, nba2021per36 %>% pull(Pos)),
rf_OOB_output(data_fit_mtry4,4, nba2021per36 %>% pull(Pos)),
rf_OOB_output(data_fit_mtry9,9, nba2021per36 %>% pull(Pos)),
rf_OOB_output(data_fit_mtry19,19, nba2021per36 %>% pull(Pos))
)
data_rf_OOB_output %>%
group_by(label) %>%
accuracy(truth = class, estimate = .pred_class)
rf_OOB_output(data_fit_mtry2,2, nba2021per36 %>% pull(Pos)) %>%
conf_mat(truth = class, estimate= .pred_class)
# Evaluate OOB Metrics
data_rf_OOB_output <- bind_rows(
rf_OOB_output(data_fit_mtry2,2, nba2021per36 %>% pull(Pos)),
rf_OOB_output(data_fit_mtry4,4, nba2021per36 %>% pull(Pos)),
rf_OOB_output(data_fit_mtry9,9, nba2021per36 %>% pull(Pos)),
rf_OOB_output(data_fit_mtry19,19, nba2021per36 %>% pull(Pos))
)
data_rf_OOB_output %>%
group_by(label) %>%
accuracy(truth = class, estimate = .pred_class)
# Evaluate OOB Metrics
rf_OOB_output <- function(fit_model, model_label, truth){
tibble(
.pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'), #OOB predictions
class = truth,
label = model_label
)
}
data_rf_OOB_output <- bind_rows(
rf_OOB_output(data_fit_mtry2,2, nba2021per36 %>% pull(Pos)),
rf_OOB_output(data_fit_mtry4,4, nba2021per36 %>% pull(Pos)),
rf_OOB_output(data_fit_mtry9,9, nba2021per36 %>% pull(Pos)),
rf_OOB_output(data_fit_mtry19,19, nba2021per36 %>% pull(Pos))
)
data_rf_OOB_output %>%
group_by(label) %>%
accuracy(truth = class, estimate = .pred_class)
rf_OOB_output(data_fit_mtry2,2, nba2021per36 %>% pull(Pos)) %>%
conf_mat(truth = class, estimate= .pred_class)
rf_OOB_output(data_fit_mtry4,4, nba2021per36 %>% pull(Pos)) %>%
conf_mat(truth = class, estimate= .pred_class)
rf_OOB_output(data_fit_mtry9,9, nba2021per36 %>% pull(Pos)) %>%
conf_mat(truth = class, estimate= .pred_class)
rf_OOB_output(data_fit_mtry19,19, nba2021per36 %>% pull(Pos)) %>%
conf_mat(truth = class, estimate= .pred_class)
nba2021per36_sub <- nba2021per36 %>%
select(STL, BLK, AST, TRB, FT., X3PA)
# Run k-means for k = centers = 3
set.seed(253)
kclust_k3 <- kmeans(nba2021per36_sub, centers = 3)
# Display the cluter assignments
kclust_k3$cluster
nba2021per36 <- nba2021per36 %>%
mutate(kclust_3 = factor(kclust_k3$cluster))
set.seed(253)
kclust_k3_scale <- kmeans(scale(nba2021per36_sub), centers = 3)
nba2021per36 <- nba2021per36 %>%
mutate(kclust_3_scale = factor(kclust_k3_scale$cluster))
# Visualize the new cluster assignments
ggplot(nba2021per36, aes(x=`BLK`, y= `AST`, color=kclust_3_scale)) +
geom_point()
model_output <-data_fit_mtry12 %>%
extract_fit_engine()
model_output <-data_fit_mtry12 %>%
extract_fit_engine()
model_output <-data_fit_mtry12 %>%
extract_fit_engine()
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)
library(dplyr)
library(readr)
library(ggplot2)
library(vip)
library(tidymodels)
tidymodels_prefer()
conflicted::conflict_prefer("vi", "vip")
# Read in the data
land <- read_csv("https://www.macalester.edu/~ajohns24/data/land_cover.csv")
# There are 9 land types, but we'll focus on 3 of them
land <- land %>%
filter(class %in% c("asphalt", "grass", "tree")) %>%
mutate(class = factor(class))
# Make sure you understand what each line of code is doing
# Model Specification
rf_spec <- rand_forest() %>%
set_engine(engine = 'ranger') %>%
set_args(mtry = NULL, # size of random subset of variables; default is floor(sqrt(number of total predictors))
trees = 1000, # Number of trees
min_n = 2,
probability = FALSE, # FALSE: get hard predictions (not needed for regression)
importance = 'impurity') %>% # we'll come back to this at the end
set_mode('classification') # change this for regression
# Recipe
data_rec <- recipe(class ~ ., data = land)
# Workflows
data_wf_mtry2 <- workflow() %>%
add_model(rf_spec %>% set_args(mtry = 2)) %>%
add_recipe(data_rec)
## Create workflows for mtry = 12, 74, and 147
data_wf_mtry12 <- workflow() %>%
add_model(rf_spec %>% set_args(mtry = 12)) %>%
add_recipe(data_rec)
data_wf_mtry74 <- workflow() %>%
add_model(rf_spec %>% set_args(mtry = 74)) %>%
add_recipe(data_rec)
data_wf_mtry147 <- workflow() %>%
add_model(rf_spec %>% set_args(mtry = 147)) %>%
add_recipe(data_rec)
# Fit Models
set.seed(123) # make sure to run this before each fit so that you have the same 1000 trees
data_fit_mtry2 <- fit(data_wf_mtry2, data = land)
# Fit models for 12, 74, 147
set.seed(123)
data_fit_mtry12 <- fit(data_wf_mtry12, data = land)
set.seed(123)
data_fit_mtry74 <- fit(data_wf_mtry74, data = land)
set.seed(123)
data_fit_mtry147 <- fit(data_wf_mtry147, data = land)
# Custom Function to get OOB predictions, true observed outcomes and add a user-provided model label
rf_OOB_output <- function(fit_model, model_label, truth){
tibble(
.pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'), #OOB predictions
class = truth,
label = model_label
)
}
#check out the function output
rf_OOB_output(data_fit_mtry2,2, land %>% pull(class))
# Evaluate OOB Metrics
data_rf_OOB_output <- bind_rows(
rf_OOB_output(data_fit_mtry2,2, land %>% pull(class)),
rf_OOB_output(data_fit_mtry12,12, land %>% pull(class)),
rf_OOB_output(data_fit_mtry74,74, land %>% pull(class)),
rf_OOB_output(data_fit_mtry147,147, land %>% pull(class))
)
data_rf_OOB_output %>%
group_by(label) %>%
accuracy(truth = class, estimate = .pred_class)
data_fit_mtry12
rf_OOB_output(data_fit_mtry12,12, land %>% pull(class)) %>%
conf_mat(truth = class, estimate= .pred_class)
model_output <-data_fit_mtry12 %>%
extract_fit_engine()
model_output %>%
vip(num_features = 30) + theme_classic() #based on impurity
model_output %>% vip::vi() %>% head()
model_output %>% vip::vi() %>% tail()
model_output2 <- data_wf_mtry12 %>%
update_model(rf_spec %>% set_args(importance = "permutation")) %>% #based on permutation
fit(data = land) %>%
extract_fit_engine()
model_output2 %>%
vip(num_features = 30) + theme_classic()
model_output2 %>% vip::vi() %>% head()
model_output2 %>% vip::vi() %>% tail()
model_output <-data_fit_mtry4 %>%
extract_fit_engine()
model_output %>%
vip(num_features = 30) + theme_classic() #based on impurity
model_output %>% vip::vi() %>% head()
model_output %>% vip::vi() %>% tail()
model_output2 <- data_wf_mtry4 %>%
update_model(rf_spec %>% set_args(importance = "permutation")) %>% #based on permutation
fit(data = land) %>%
extract_fit_engine()
model_output2 <- data_wf_mtry4 %>%
update_model(rf_spec %>% set_args(importance = "permutation")) %>% #based on permutation
fit(data = nba2021per36) %>%
extract_fit_engine()
model_output2 %>%
vip(num_features = 30) + theme_classic()
model_output2 %>% vip::vi() %>% head()
model_output2 %>% vip::vi() %>% tail()
coefs_lambdas %>%
ggplot(aes(x = lambda, y = coef, group = term, color = var)) +
geom_line() +
geom_vline(xintercept = best_se_penalty %>% pull(penalty), linetype = 'dashed') +
theme(legend.position = "bottom", legend.text=element_text(size=8)) +
xlim(0,7700000) + ylim(-2500000,5000000)
library(dplyr)
library(readr)
library(ggplot2)
library(rpart.plot)
library(tidymodels)
tidymodels_prefer()
# Read in the data
land <- read_csv("https://www.macalester.edu/~ajohns24/data/land_cover.csv")
# There are 9 land types, but we'll focus on 3 of them
land <- land %>%
filter(class %in% c("asphalt", "grass", "tree"))
# Make sure you understand what each line of code is doing
set.seed(123) # don't change this
data_fold <- vfold_cv(land, v = 10)
ct_spec_tune <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = tune(),
min_n = 2,
tree_depth = NULL) %>%
set_mode('classification')
data_rec <- recipe(class ~ ., data = land)
data_wf_tune <- workflow() %>%
add_model(ct_spec_tune) %>%
add_recipe(data_rec)
param_grid <- grid_regular(cost_complexity(range = c(-5, -1)), levels = 10)
tune_res <- tune_grid(
data_wf_tune,
resamples = data_fold,
grid = param_grid,
metrics = metric_set(accuracy) #change this for regression trees
)
best_complexity <- select_by_one_std_err(tune_res, metric = 'accuracy', desc(cost_complexity))
best_complexity %>%
pull(cost_complexity)
data_wf_final <- finalize_workflow(data_wf_tune, best_complexity)
land_final_fit <- fit(data_wf_final, data = land)
tune_res %>%
collect_metrics() %>%
filter(cost_complexity == best_complexity %>% pull(cost_complexity))
tree_mod_lowcp <- fit(
data_wf_tune %>%
update_model(ct_spec_tune %>% set_args(cost_complexity = .00001)),
data = land
)
tree_mod_highcp <- fit(
data_wf_tune %>%
update_model(ct_spec_tune %>% set_args(cost_complexity = .1)),
data = land
)
# Plot all 3 trees in a row
#par(mfrow = c(1,3))
tree_mod_lowcp %>% extract_fit_engine() %>% rpart.plot()
land_final_fit %>% extract_fit_engine() %>% rpart.plot()
tree_mod_highcp %>% extract_fit_engine() %>% rpart.plot()
# Pick out training case 2 to make a prediction
test_case <- land[2,]
# Show only the needed predictors
test_case %>% select(NDVI, Bright_100, SD_NIR, GLCM2_100)
land_final_fit %>% extract_fit_engine() %>% rpart.plot()
# Soft (probability) prediction
predict(land_final_fit, new_data = test_case, type = "prob")
# Hard (class) prediction
predict(land_final_fit, new_data = test_case, type = "class")
autoplot(tune_res) + theme_classic()
# Make sure you understand what each line of code is doing
set.seed(123) # don't change this
data_fold <- vfold_cv(land, v = 10)
ct_spec_tune <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = tune(),
min_n = 2,
tree_depth = NULL) %>%
set_mode('classification')
data_rec <- recipe(class ~ ., data = land)
data_wf_tune <- workflow() %>%
add_model(ct_spec_tune) %>%
add_recipe(data_rec)
param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)
tune_res <- tune_grid(
data_wf_tune,
resamples = data_fold,
grid = param_grid,
metrics = metric_set(accuracy) #change this for regression trees
)
autoplot(tune_res) + theme_classic()
best_complexity <- select_by_one_std_err(tune_res, metric = 'accuracy', desc(cost_complexity))
best_complexity %>%
pull(cost_complexity)
data_wf_final <- finalize_workflow(data_wf_tune, best_complexity)
land_final_fit <- fit(data_wf_final, data = land)
tune_res %>%
collect_metrics() %>%
filter(cost_complexity == best_complexity %>% pull(cost_complexity))
# Make sure you understand what each line of code is doing
set.seed(123) # don't change this
data_fold <- vfold_cv(nba2021per36, v = 10)
ct_spec_tune <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = tune(),
min_n = 2,
tree_depth = NULL) %>%
set_mode('classification')
data_rec <- recipe(Pos ~ FG +FGA + FT. + PTS +STL + BLK + ORB + DRB + AST + X3P. + X2P. + TOV, data = nba2021per36)
data_wf_tune <- workflow() %>%
add_model(ct_spec_tune) %>%
add_recipe(data_rec)
param_grid <- grid_regular(cost_complexity(range = c(-5, -1)), levels = 10)
tune_res <- tune_grid(
data_wf_tune,
resamples = data_fold,
grid = param_grid,
metrics = metric_set(accuracy) #change this for regression trees
)
best_complexity <- select_by_one_std_err(tune_res, metric = 'accuracy', desc(cost_complexity))
best_complexity %>%
pull(cost_complexity)
data_wf_final <- finalize_workflow(data_wf_tune, best_complexity)
nba_final_fit <- fit(data_wf_final, data = nba2021per36)
tune_res %>%
collect_metrics() %>%
filter(cost_complexity == best_complexity %>% pull(cost_complexity))
tree_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = tune(),
min_n = 2,
tree_depth = NULL) %>%
set_mode('classification')
nba2021per36 <- nba2021per36 %>%
mutate(Pos = stringr::str_replace(stringr::str_sub(Pos,1,2),"-",""))
nba2021per36 <- nba2021per36 %>%
mutate(Pos = as.factor(Pos))
data_fold <- vfold_cv(nba2021per36, v = 10)
data_rec <- recipe(Pos ~ FG +FGA + FT. + PTS +STL + BLK + ORB + DRB + AST + X3P. + X2P. + TOV, data = nba2021per36)
data_wf_tune <- workflow() %>%
add_model(tree_spec) %>%
add_recipe(data_rec)
param_grid <- grid_regular(cost_complexity(range = c(-5, -1)), levels = 10)
tune_res <- tune_grid(
data_wf_tune,
resamples = data_fold,
grid = param_grid,
metrics = metric_set(accuracy) #change this for regression trees
)
autoplot(tune_res) + theme_classic()
best_complexity <- select_by_one_std_err(tune_res, metric = 'accuracy', desc(cost_complexity))
best_complexity %>%
pull(cost_complexity)
data_wf_final <- finalize_workflow(data_wf_tune, best_complexity)
pos_final_fit <- fit(data_wf_final, data = nba2021per36)
tune_res %>%
collect_metrics() %>%
filter(cost_complexity == best_complexity %>% pull(cost_complexity))
tree_mod_lowcp <- fit(
data_wf_tune %>%
update_model(tree_spec %>% set_args(cost_complexity = .012)),
data = nba2021per36
)
tree_mod_highcp <- fit(
data_wf_tune %>%
update_model(tree_spec %>% set_args(cost_complexity = .018)),
data = nba2021per36
)
# Plot all 3 trees in a row
#par(mfrow = c(1,3))
tree_mod_lowcp %>% extract_fit_engine() %>% rpart.plot()
pos_final_fit %>% extract_fit_engine() %>% rpart.plot()
tree_mod_highcp %>% extract_fit_engine() %>% rpart.plot()
tree_mod_lowcp %>%
collect_metrics()
tree_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = tune(),
min_n = 2,
tree_depth = NULL) %>%
set_mode('classification')
nba2021per36 <- nba2021per36 %>%
mutate(Pos = stringr::str_replace(stringr::str_sub(Pos,1,2),"-",""))
nba2021per36 <- nba2021per36 %>%
mutate(Pos = as.factor(Pos))
data_fold <- vfold_cv(nba2021per36, v = 10)
data_rec <- recipe(Pos ~ FG +FGA + FT. + PTS +STL + BLK + ORB + DRB + AST + X3P. + X2P. + TOV, data = nba2021per36)
data_wf_tune <- workflow() %>%
add_model(tree_spec) %>%
add_recipe(data_rec)
param_grid <- grid_regular(cost_complexity(range = c(-5, -1)), levels = 10)
tune_res <- tune_grid(
data_wf_tune,
resamples = data_fold,
grid = param_grid,
metrics = metric_set(accuracy) #change this for regression trees
)
autoplot(tune_res) + theme_classic()
best_complexity <- select_by_one_std_err(tune_res, metric = 'accuracy', desc(cost_complexity))
best_complexity %>%
pull(cost_complexity)
data_wf_final <- finalize_workflow(data_wf_tune, best_complexity)
pos_final_fit <- fit(data_wf_final, data = nba2021per36)
tune_res %>%
collect_metrics() %>%
filter(cost_complexity == best_complexity %>% pull(cost_complexity))
tree_mod_lowcp <- fit(
data_wf_tune %>%
update_model(tree_spec %>% set_args(cost_complexity = .012)),
data = nba2021per36
)
tree_mod_highcp <- fit(
data_wf_tune %>%
update_model(tree_spec %>% set_args(cost_complexity = .018)),
data = nba2021per36
)
# Plot all 3 trees in a row
#par(mfrow = c(1,3))
tree_mod_lowcp %>% extract_fit_engine() %>% rpart.plot()
pos_final_fit %>% extract_fit_engine() %>% rpart.plot()
tree_mod_highcp %>% extract_fit_engine() %>% rpart.plot()
tree_mod_highcp <- fit(
data_wf_tune %>%
update_model(tree_spec %>% set_args(cost_complexity = .035)),
data = nba2021per36
)
tree_mod_highcp %>% extract_fit_engine() %>% rpart.plot()
