step_nzv(all_predictors()) %>%
step_novel(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors())
knn_wf <- workflow() %>%
add_model(knn_spec) %>%
add_recipe(knn_rec)
# fit & tune models
penalty_grid <- grid_regular(
penalty(range = c(3, 7)), #log10 transformed
levels = 10)
nbatune_output <- tune_grid( # new function for tuning parameters
lasso_wf_tune, # workflow
resamples = nba_cv10, # cv folds
metrics = metric_set(mae),
grid = penalty_grid # penalty grid defined above
)
nbatune_output
autoplot(nbatune_output)
mod1 %>% tidy()
#  calculate/collect CV metrics
best_penalty <- select_best(nbatune_output, metric = 'mae')
#best_penalty
best_se_penalty <- select_by_one_std_err(nbatune_output, metric = 'mae', desc(penalty)) # choose largest penalty value within 1 se of the lowest cv mae
best_se_penalty
final_wf <- finalize_workflow(lasso_wf_tune, best_penalty) # incorporates penalty value to workflow
final_wf_se <- finalize_workflow(lasso_wf_tune, best_se_penalty) # incorporates penalty value to workflow
final_fit <- fit(final_wf, data = nba2021pergame)
final_fit_se <- fit(final_wf_se, data = nba2021pergame)
#tidy(final_fit)
tidy(final_fit_se)
glmnet_output <- final_fit_se %>% extract_fit_parsnip() %>% pluck('fit') # get the original glmnet output
lambdas <- glmnet_output$lambda
coefs_lambdas <-
coefficients(glmnet_output, s = lambdas )  %>%
as.matrix() %>%
t() %>%
as.data.frame() %>%
mutate(lambda = lambdas ) %>%
select(lambda, everything(), -`(Intercept)`) %>%
pivot_longer(cols = -lambda,
names_to = "term",
values_to = "coef") %>%
mutate(var = purrr::map_chr(stringr::str_split(term,"_"),~.[1]))
coefs_lambdas %>%
ggplot(aes(x = lambda, y = coef, group = term, color = var)) +
geom_line() +
geom_vline(xintercept = best_se_penalty %>% pull(penalty), linetype = 'dashed') +
theme_classic() +
theme(legend.position = "bottom", legend.text=element_text(size=8)) +
xlim(0,8000000) + ylim(-2000000,5000000)
# recipes & workflows
mod1 <- fit(lm_spec,
Salary ~ G+GS +FG +FGA + MP + PTS +STL + BLK + Age + ORB + DRB + AST + X3P. + X2P., data = nba2021pergame)
full_rec <- recipe(Salary ~ G+ GS +FG +FGA + MP + PTS +STL + BLK + Age + TOV + PF + ORB + DRB + TRB + AST + X3P + X3PA + X2P + X2PA + FT + FTA + FT., data = nba2021pergame) %>%
step_nzv(all_predictors()) %>% # removes variables with the same value
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors())
lasso_wf_tune <- workflow() %>%
add_recipe(full_rec) %>% # recipe defined above
add_model(lm_lasso_spec_tune)
knn_rec <- recipe(Salary ~ . , data = nba2021pergame) %>%
step_nzv(all_predictors()) %>%
step_novel(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors())
knn_wf <- workflow() %>%
add_model(knn_spec) %>%
add_recipe(knn_rec)
# fit & tune models
penalty_grid <- grid_regular(
penalty(range = c(3, 7)), #log10 transformed
levels = 10)
nbatune_output <- tune_grid( # new function for tuning parameters
lasso_wf_tune, # workflow
resamples = nba_cv10, # cv folds
metrics = metric_set(mae),
grid = penalty_grid # penalty grid defined above
)
nbatune_output
autoplot(nbatune_output)
mod1 %>% tidy()
#  calculate/collect CV metrics
best_penalty <- select_best(nbatune_output, metric = 'mae')
#best_penalty
best_se_penalty <- select_by_one_std_err(nbatune_output, metric = 'mae', desc(penalty)) # choose largest penalty value within 1 se of the lowest cv mae
best_se_penalty
final_wf <- finalize_workflow(lasso_wf_tune, best_penalty) # incorporates penalty value to workflow
final_wf_se <- finalize_workflow(lasso_wf_tune, best_se_penalty) # incorporates penalty value to workflow
final_fit <- fit(final_wf, data = nba2021pergame)
final_fit_se <- fit(final_wf_se, data = nba2021pergame)
#tidy(final_fit)
tidy(final_fit_se)
glmnet_output <- final_fit_se %>% extract_fit_parsnip() %>% pluck('fit') # get the original glmnet output
lambdas <- glmnet_output$lambda
coefs_lambdas <-
coefficients(glmnet_output, s = lambdas )  %>%
as.matrix() %>%
t() %>%
as.data.frame() %>%
mutate(lambda = lambdas ) %>%
select(lambda, everything(), -`(Intercept)`) %>%
pivot_longer(cols = -lambda,
names_to = "term",
values_to = "coef") %>%
mutate(var = purrr::map_chr(stringr::str_split(term,"_"),~.[1]))
coefs_lambdas %>%
ggplot(aes(x = lambda, y = coef, group = term, color = var)) +
geom_line() +
geom_vline(xintercept = best_se_penalty %>% pull(penalty), linetype = 'dashed') +
theme_classic() +
theme(legend.position = "bottom", legend.text=element_text(size=8)) +
xlim(0,8000000) + ylim(-2000000,5000000)
lm_lasso_spec_tune <-
linear_reg() %>%
set_args(mixture = 1, penalty = tune()) %>% ## mixture = 1 indicates Lasso
set_engine(engine = 'glmnet') %>% #note we are using a different engine
set_mode('regression')
mod1 <- fit(lm_spec,
Salary ~ G+GS +FG +FGA + MP + PTS +STL + BLK + Age + ORB + DRB + AST + X3P. + X2P., data = nba2021pergame)
full_rec <- recipe(Salary ~ G+ GS +FG +FGA + MP + PTS +STL + BLK + Age + TOV + PF + ORB + DRB + TRB + AST + X3P + X3PA + X2P + X2PA + FT + FTA + FT., data = nba2021pergame) %>%
step_nzv(all_predictors()) %>% # removes variables with the same value
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors())
penalty_grid <- grid_regular(
penalty(range = c(3, 7)), #log10 transformed
levels = 10)
nbatune_output <- tune_grid( # new function for tuning parameters
lasso_wf_tune, # workflow
resamples = nba_cv10, # cv folds
metrics = metric_set(mae),
grid = penalty_grid # penalty grid defined above
)
nbatune_output
mod1 %>% tidy()
best_se_penalty <- select_by_one_std_err(nbatune_output, metric = 'mae', desc(penalty))
best_se_penalty
final_wf_se <- finalize_workflow(lasso_wf_tune, best_se_penalty) # incorporates penalty value to workflow
final_fit_se <- fit(final_wf_se, data = nba2021pergame)
tidy(final_fit_se)
glmnet_output <- final_fit_se %>% extract_fit_parsnip() %>% pluck('fit') # get the original glmnet output
lambdas <- glmnet_output$lambda
coefs_lambdas <-
coefficients(glmnet_output, s = lambdas )  %>%
as.matrix() %>%
t() %>%
as.data.frame() %>%
mutate(lambda = lambdas ) %>%
select(lambda, everything(), -`(Intercept)`) %>%
pivot_longer(cols = -lambda,
names_to = "term",
values_to = "coef") %>%
mutate(var = purrr::map_chr(stringr::str_split(term,"_"),~.[1]))
coefs_lambdas %>%
ggplot(aes(x = lambda, y = coef, group = term, color = var)) +
geom_line() +
geom_vline(xintercept = best_se_penalty %>% pull(penalty), linetype = 'dashed') +
theme_classic() +
theme(legend.position = "bottom", legend.text=element_text(size=8)) +
xlim(0,8000000) + ylim(-2000000,5000000)
lm_lasso_spec_tune <-
linear_reg() %>%
set_args(mixture = 1, penalty = tune()) %>%
set_engine(engine = 'glmnet') %>%
set_mode('regression')
mod1 <- fit(lm_spec,
Salary ~ G+GS +FG +FGA + MP + PTS +STL + BLK + Age + ORB + DRB + AST + X3P. + X2P., data = nba2021pergame)
full_rec <- recipe(Salary ~ G+ GS +FG +FGA + MP + PTS +STL + BLK + Age + TOV + PF + ORB + DRB + TRB + AST + X3P + X3PA + X2P + X2PA + FT + FTA + FT., data = nba2021pergame) %>%
step_nzv(all_predictors()) %>% # removes variables with the same value
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors())
penalty_grid <- grid_regular(
penalty(range = c(3, 7)), #log10 transformed
levels = 10)
nbatune_output <- tune_grid( # new function for tuning parameters
lasso_wf_tune, # workflow
resamples = nba_cv10, # cv folds
metrics = metric_set(mae),
grid = penalty_grid # penalty grid defined above
)
mod1 %>% tidy()
best_se_penalty <- select_by_one_std_err(nbatune_output, metric = 'mae', desc(penalty))
best_se_penalty
final_wf_se <- finalize_workflow(lasso_wf_tune, best_se_penalty) # incorporates penalty value to workflow
final_fit_se <- fit(final_wf_se, data = nba2021pergame)
tidy(final_fit_se)
glmnet_output <- final_fit_se %>% extract_fit_parsnip() %>% pluck('fit') # get the original glmnet output
lambdas <- glmnet_output$lambda
coefs_lambdas <-
coefficients(glmnet_output, s = lambdas )  %>%
as.matrix() %>%
t() %>%
as.data.frame() %>%
mutate(lambda = lambdas ) %>%
select(lambda, everything(), -`(Intercept)`) %>%
pivot_longer(cols = -lambda,
names_to = "term",
values_to = "coef") %>%
mutate(var = purrr::map_chr(stringr::str_split(term,"_"),~.[1]))
coefs_lambdas %>%
ggplot(aes(x = lambda, y = coef, group = term, color = var)) +
geom_line() +
geom_vline(xintercept = best_se_penalty %>% pull(penalty), linetype = 'dashed') +
theme_classic() +
theme(legend.position = "bottom", legend.text=element_text(size=8)) +
xlim(0,8000000) + ylim(-2000000,5000000)
lm_lasso_spec_tune <-
linear_reg() %>%
set_args(mixture = 1, penalty = tune()) %>%
set_engine(engine = 'glmnet') %>%
set_mode('regression')
full_rec <- recipe(Salary ~ G+ GS +FG +FGA + MP + PTS +STL + BLK + Age + TOV + PF + ORB + DRB + TRB + AST + X3P + X3PA + X2P + X2PA + FT + FTA + FT., data = nba2021pergame) %>%
step_nzv(all_predictors()) %>% # removes variables with the same value
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors())
lasso_wf_tune <- workflow() %>%
add_recipe(full_rec) %>% # recipe defined above
add_model(lm_lasso_spec_tune)
penalty_grid <- grid_regular(
penalty(range = c(3, 7)), #log10 transformed
levels = 10)
nbatune_output <- tune_grid( # new function for tuning parameters
lasso_wf_tune, # workflow
resamples = nba_cv10, # cv folds
metrics = metric_set(mae),
grid = penalty_grid # penalty grid defined above
)
best_se_penalty <- select_by_one_std_err(nbatune_output, metric = 'mae', desc(penalty))
best_se_penalty
final_wf_se <- finalize_workflow(lasso_wf_tune, best_se_penalty) # incorporates penalty value to workflow
final_fit_se <- fit(final_wf_se, data = nba2021pergame)
tidy(final_fit_se)
glmnet_output <- final_fit_se %>% extract_fit_parsnip() %>% pluck('fit') # get the original glmnet output
lambdas <- glmnet_output$lambda
coefs_lambdas <-
coefficients(glmnet_output, s = lambdas )  %>%
as.matrix() %>%
t() %>%
as.data.frame() %>%
mutate(lambda = lambdas ) %>%
select(lambda, everything(), -`(Intercept)`) %>%
pivot_longer(cols = -lambda,
names_to = "term",
values_to = "coef") %>%
mutate(var = purrr::map_chr(stringr::str_split(term,"_"),~.[1]))
coefs_lambdas %>%
ggplot(aes(x = lambda, y = coef, group = term, color = var)) +
geom_line() +
geom_vline(xintercept = best_se_penalty %>% pull(penalty), linetype = 'dashed') +
theme_classic() +
theme(legend.position = "bottom", legend.text=element_text(size=8)) +
xlim(0,8000000) + ylim(-2000000,5000000)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(rpart.plot)
library(tidymodels)
tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions
set.seed(123)
nba2021pergame <- read.csv("2021-PerGame-Player-Stats.csv")
nba2021_2022salaries <- read.csv("2021-2022-Player-Salaries.csv")
names(nba2021_2022salaries)[1] <- "Player"
nba2021pergame <- nba2021pergame %>% left_join(nba2021_2022salaries,by=c('Player')) %>%
select(-AllStar)  %>%
distinct(Player, .keep_all = TRUE) %>%
filter(!is.na(Salary)) %>%
na.omit()
head(nba2021pergame)
nba2021pergame %>%
ggplot(aes(x = WS, y = Salary, color = AllNBA)) +
geom_point(alpha = 0.2) +
geom_smooth(span = 0.8, se = FALSE) +
theme_classic()
lm_lasso_spec_tune <-
linear_reg() %>%
set_args(mixture = 1, penalty = tune()) %>%
set_engine(engine = 'glmnet') %>%
set_mode('regression')
full_rec <- recipe(Salary ~ G+ GS +FG +FGA + MP + PTS +STL + BLK + Age + TOV + PF + ORB + DRB + TRB + AST + X3P + X3PA + X2P + X2PA + FT + FTA + FT., data = nba2021pergame) %>%
step_nzv(all_predictors()) %>% # removes variables with the same value
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors())
lasso_wf_tune <- workflow() %>%
add_recipe(full_rec) %>% # recipe defined above
add_model(lm_lasso_spec_tune)
penalty_grid <- grid_regular(
penalty(range = c(3, 7)), #log10 transformed
levels = 10)
nbatune_output <- tune_grid( # new function for tuning parameters
lasso_wf_tune, # workflow
resamples = nba_cv10, # cv folds
metrics = metric_set(mae),
grid = penalty_grid # penalty grid defined above
)
best_se_penalty <- select_by_one_std_err(nbatune_output, metric = 'mae', desc(penalty))
best_se_penalty
final_wf_se <- finalize_workflow(lasso_wf_tune, best_se_penalty) # incorporates penalty value to workflow
final_fit_se <- fit(final_wf_se, data = nba2021pergame)
tidy(final_fit_se)
glmnet_output <- final_fit_se %>% extract_fit_parsnip() %>% pluck('fit') # get the original glmnet output
lambdas <- glmnet_output$lambda
coefs_lambdas <-
coefficients(glmnet_output, s = lambdas )  %>%
as.matrix() %>%
t() %>%
as.data.frame() %>%
mutate(lambda = lambdas ) %>%
select(lambda, everything(), -`(Intercept)`) %>%
pivot_longer(cols = -lambda,
names_to = "term",
values_to = "coef") %>%
mutate(var = purrr::map_chr(stringr::str_split(term,"_"),~.[1]))
coefs_lambdas %>%
ggplot(aes(x = lambda, y = coef, group = term, color = var)) +
geom_line() +
geom_vline(xintercept = best_se_penalty %>% pull(penalty), linetype = 'dashed') +
theme_classic() +
theme(legend.position = "bottom", legend.text=element_text(size=8)) +
xlim(0,8000000) + ylim(-2000000,5000000)
nba2021per36 <- read.csv("Per36-Player.csv")
nba2021_2022salaries <- read.csv("2021-2022-Player-Salaries.csv")
names(nba2021_2022salaries)[1] <- "Player"
nba2021per36 <- nba2021per36 %>% left_join(nba2021_2022salaries,by=c('Player')) %>%
select(-AllStar)  %>%
distinct(Player, .keep_all = TRUE) %>%
filter(!is.na(Salary)) %>%
filter(MP > 300) %>%
na.omit()
head(nba2021per36)
tree_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = tune(),
min_n = 2,
tree_depth = NULL) %>%
set_mode('classification')
nba2021per36 <- nba2021per36 %>%
mutate(Pos = stringr::str_replace(stringr::str_sub(Pos,1,2),"-",""))
nba2021per36 <- nba2021per36 %>%
mutate(Pos = as.factor(Pos))
data_fold <- vfold_cv(nba2021per36, v = 10)
data_rec <- recipe(Pos ~ FG +FGA + FT. + PTS +STL + BLK + ORB + DRB + AST + X3P. + X2P. + TOV, data = nba2021per36)
data_wf_tune <- workflow() %>%
add_model(tree_spec) %>%
add_recipe(data_rec)
param_grid <- grid_regular(cost_complexity(range = c(-5, -1)), levels = 10)
tune_res <- tune_grid(
data_wf_tune,
resamples = data_fold,
grid = param_grid,
metrics = metric_set(accuracy) #change this for regression trees
)
autoplot(tune_res) + theme_classic()
best_complexity <- select_by_one_std_err(tune_res, metric = 'accuracy', desc(cost_complexity))
best_complexity %>%
pull(cost_complexity)
data_wf_final <- finalize_workflow(data_wf_tune, best_complexity)
pos_final_fit <- fit(data_wf_final, data = nba2021per36)
tune_res %>%
collect_metrics() %>%
filter(cost_complexity == best_complexity %>% pull(cost_complexity))
tree_mod_lowcp <- fit(
data_wf_tune %>%
update_model(tree_spec %>% set_args(cost_complexity = .0001)),
data = nba2021per36
)
tree_mod_highcp <- fit(
data_wf_tune %>%
update_model(tree_spec %>% set_args(cost_complexity = .018)),
data = nba2021per36
)
# Plot all 3 trees in a row
#par(mfrow = c(1,3))
tree_mod_lowcp %>% extract_fit_engine() %>% rpart.plot()
pos_final_fit %>% extract_fit_engine() %>% rpart.plot()
tree_mod_highcp %>% extract_fit_engine() %>% rpart.plot()
nba2021per36_sub <- nba2021per36 %>%
select(STL, BLK, AST, TRB, FG., FT., X3PA)
# Run k-means for k = centers = 3
set.seed(253)
kclust_k5 <- kmeans(nba2021per36_sub, centers = 5)
# Display the cluter assignments
kclust_k5$cluster
nba2021per36 <- nba2021per36 %>%
mutate(kclust_5 = factor(kclust_k5$cluster))
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)
library(dplyr)
library(readr)
library(ggplot2)
library(vip)
library(tidymodels)
tidymodels_prefer()
conflicted::conflict_prefer("vi", "vip")
# Read in the data
land <- read_csv("https://www.macalester.edu/~ajohns24/data/land_cover.csv")
# There are 9 land types, but we'll focus on 3 of them
land <- land %>%
filter(class %in% c("asphalt", "grass", "tree")) %>%
mutate(class = factor(class))
# Make sure you understand what each line of code is doing
# Model Specification
rf_spec <- rand_forest() %>%
set_engine(engine = 'ranger') %>%
set_args(mtry = NULL, # size of random subset of variables; default is floor(sqrt(number of total predictors))
trees = 1000, # Number of trees
min_n = 2,
probability = FALSE, # FALSE: get hard predictions (not needed for regression)
importance = 'impurity') %>% # we'll come back to this at the end
set_mode('classification') # change this for regression
# Recipe
data_rec <- recipe(class ~ ., data = land)
# Workflows
data_wf_mtry2 <- workflow() %>%
add_model(rf_spec %>% set_args(mtry = 2)) %>%
add_recipe(data_rec)
## Create workflows for mtry = 12, 74, and 147
data_wf_mtry12 <- workflow() %>%
add_model(rf_spec %>% set_args(mtry = 12)) %>%
add_recipe(data_rec)
data_wf_mtry74 <- workflow() %>%
add_model(rf_spec %>% set_args(mtry = 74)) %>%
add_recipe(data_rec)
data_wf_mtry147 <- workflow() %>%
add_model(rf_spec %>% set_args(mtry = 147)) %>%
add_recipe(data_rec)
# Fit Models
set.seed(123) # make sure to run this before each fit so that you have the same 1000 trees
data_fit_mtry2 <- fit(data_wf_mtry2, data = land)
# Fit models for 12, 74, 147
set.seed(123)
data_fit_mtry12 <- fit(data_wf_mtry12, data = land)
set.seed(123)
data_fit_mtry74 <- fit(data_wf_mtry74, data = land)
set.seed(123)
data_fit_mtry147 <- fit(data_wf_mtry147, data = land)
# Custom Function to get OOB predictions, true observed outcomes and add a user-provided model label
rf_OOB_output <- function(fit_model, model_label, truth){
tibble(
.pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'), #OOB predictions
class = truth,
label = model_label
)
}
#check out the function output
rf_OOB_output(data_fit_mtry2,2, land %>% pull(class))
# Evaluate OOB Metrics
data_rf_OOB_output <- bind_rows(
rf_OOB_output(data_fit_mtry2,2, land %>% pull(class)),
rf_OOB_output(data_fit_mtry12,12, land %>% pull(class)),
rf_OOB_output(data_fit_mtry74,74, land %>% pull(class)),
rf_OOB_output(data_fit_mtry147,147, land %>% pull(class))
)
data_rf_OOB_output %>%
group_by(label) %>%
accuracy(truth = class, estimate = .pred_class)
data_fit_mtry12
rf_OOB_output(data_fit_mtry12,12, land %>% pull(class)) %>%
conf_mat(truth = class, estimate= .pred_class)
View(nba2021per36)
library(vip)
conflicted::conflict_prefer("vi", "vip")
rf_spec <- rand_forest() %>%
set_engine(engine = 'ranger') %>%
set_args(mtry = NULL, # size of random subset of variables; default is floor(sqrt(number of total predictors))
trees = 1000, # Number of trees
min_n = 2,
probability = FALSE, # FALSE: get hard predictions (not needed for regression)
importance = 'impurity') %>% # we'll come back to this at the end
set_mode('classification') # change this for regression
# Recipe
data_rec <- recipe(Pos ~ FG +FGA + PTS +STL + BLK + Age + TOV + PF + ORB + DRB + TRB + AST + X3P + X3PA + X2P + X2PA + FT + FTA + FT., data = nba2021per36)
# Workflows
data_wf_mtry2 <- workflow() %>%
add_model(rf_spec %>% set_args(mtry = 2)) %>%
add_recipe(data_rec)
## Create workflows for mtry = 12, 74, and 147
data_wf_mtry12 <- workflow() %>%
add_model(rf_spec %>% set_args(mtry = 12)) %>%
add_recipe(data_rec)
data_wf_mtry74 <- workflow() %>%
add_model(rf_spec %>% set_args(mtry = 74)) %>%
add_recipe(data_rec)
data_wf_mtry147 <- workflow() %>%
add_model(rf_spec %>% set_args(mtry = 147)) %>%
add_recipe(data_rec)
# Fit Models
set.seed(123) # make sure to run this before each fit so that you have the same 1000 trees
data_fit_mtry2 <- fit(data_wf_mtry2, data = nba2021per36)
# Fit models for 12, 74, 147
set.seed(123)
data_fit_mtry12 <- fit(data_wf_mtry12, data = nba2021per36)
set.seed(123)
data_fit_mtry74 <- fit(data_wf_mtry74, data = nba2021per36)
set.seed(123)
data_fit_mtry147 <- fit(data_wf_mtry147, data = nba2021per36)
#Logistic
# Make sure you set reference level (to the outcome you are NOT interested in)
nba2021pergame <- nba2021pergame %>%
mutate(nba2021pergame = relevel(factor(AllNBA), ref='0')) #set reference level
# Logistic Regression Model Spec
logistic_spec <- logistic_reg() %>%
set_engine('glm') %>%
set_mode('classification')
# Recipe
logistic_rec <- recipe(AllNBA ~ G+GS +FG +FGA + MP + PTS +STL + BLK + Age + ORB + DRB + AST + WS + X3P. + X2P., data = nba2021pergame)
# Workflow (Recipe + Model)
log_wf <- workflow() %>%
add_recipe(logistic_rec) %>%
add_model(logistic_spec)
# Fit Model
log_fit <- fit(log_wf, data = nba2021pergame)
