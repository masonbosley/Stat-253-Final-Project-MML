# library statements
# read in data
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(tidymodels)
tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions
set.seed(123)
nba2021pergame <- read.csv("nba2021_per_game.csv")
View(nba2021pergame)
# model spec
lm_spec <-
linear_reg() %>%
set_engine(engine = 'lm') %>%
set_mode('regression')
lm_lasso_spec_tune <-
linear_reg() %>%
set_args(mixture = 1, penalty = tune()) %>% ## mixture = 1 indicates Lasso
set_engine(engine = 'glmnet') %>% #note we are using a different engine
set_mode('regression')
View(lm_lasso_spec_tune)
install.packages("splines")
install.packages("splines")
install.packages("splines")
# library statements
# read in data
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(tidymodels)
tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions
set.seed(123)
nba2021pergame <- read.csv("2021-PerGame-Player-Stats.csv")
# library statements
# read in data
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(tidymodels)
tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions
set.seed(123)
nba2021pergame <- read.csv("2021-PerGame-Player-Stats.csv")
head(nba2021pergame)
# data cleaning
nba2021pergame %>%
filter(WS > 8)
nba2021pergame
# data cleaning
bigwin <- nba2021pergame %>%
filter(WS > 8)
bigwin
# data cleaning
bigwin <- nba2021pergame %>%
filter(WS > 7.8)
bigwin
# data cleaning
bigwin <- nba2021pergame %>%
filter(WS > 7.7)
bigwin
# data cleaning
nbaclean <- nba2021pergame %>%
filter(-ï..Rk)%>%
distinct(Player)
# data cleaning
nbaclean <- nba2021pergame %>%
distinct(Player)
nbaclean
# data cleaning
nbaclean <- nba2021pergame %>%
distinct(Player, .keep_all = TRUE)
nbaclean
# data cleaning
nbaclean <- nba2021pergame %>%
distinct(Player, .keep_all = TRUE) %>%
filter(-ï..Rk)
# data cleaning
nbaclean <- nba2021pergame %>%
distinct(Player, .keep_all = TRUE) %>%
filter(-(ï..Rk)
nbaclean
# data cleaning
nbaclean <- nba2021pergame %>%
distinct(Player, .keep_all = TRUE) %>%
filter(-ï..Rk)
# data cleaning
nbaclean <- nba2021pergame %>%
distinct(Player, .keep_all = TRUE) %>%
filter(-Rk)
# data cleaning
nbaclean <- nba2021pergame %>%
distinct(Player, .keep_all = TRUE) %>%
select(-ï..Rk)
nbaclean
# creation of cv folds
nba_cv10 <- vfold_cv(nbaclean, v = 10)
# data cleaning
nbaclean <- nba2021pergame %>%
distinct(Player, .keep_all = TRUE) %>%
select(-ï..Rk)
head(nbaclean)
# data cleaning
nbaclean <- nba2021pergame %>%
distinct(Player, .keep_all = TRUE) %>%
select(-ï..Rk) %>%
filter(Pos == 'PG' | Pos == 'SG')
head(nbaclean)
# library statements
# read in data
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(tidymodels)
tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions
set.seed(123)
nba2021pergame <- read.csv("2021-PerGame-Player-Stats.csv")
head(nba2021pergame)
# data cleaning
nba_clean <-
nba2021pergame %>% distinct(Player, .keep_all = TRUE) %>%
select(-ï..Rk)
# recipes & workflows
full_rec <- recipe(AllNBA ~ ., data = nba_clean) %>%
update_role(model, new_role = 'ID') %>% # we want to keep the name of the car model but not as a predictor or outcome
step_nzv(all_predictors()) %>% # removes variables with the same value
step_normalize(all_numeric_predictors())
# recipes & workflows
full_rec <- recipe(AllNBA ~ ., data = nba_clean) %>%
step_nzv(all_predictors()) %>% # removes variables with the same value
step_normalize(all_numeric_predictors())
lasso_wf_tune <- workflow() %>%
add_recipe(full_rec) %>% # recipe defined above
add_model(lm_lasso_spec_tune)
# model spec
lm_spec <-
linear_reg() %>%
set_engine(engine = 'lm') %>%
set_mode('regression')
lm_lasso_spec_tune <-
linear_reg() %>%
set_args(mixture = 1, penalty = tune()) %>% ## mixture = 1 indicates Lasso
set_engine(engine = 'glmnet') %>% #note we are using a different engine
set_mode('regression')
# recipes & workflows
full_rec <- recipe(AllNBA ~ ., data = nba_clean) %>%
step_nzv(all_predictors()) %>% # removes variables with the same value
step_normalize(all_numeric_predictors())
lasso_wf_tune <- workflow() %>%
add_recipe(full_rec) %>% # recipe defined above
add_model(lm_lasso_spec_tune)
# fit & tune models
penalty_grid <- grid_regular(
penalty(range = c(-3, 1)), #log10 transformed
levels = 30)
tune_output <- tune_grid( # new function for tuning parameters
lasso_wf_tune, # workflow
resamples = data_cv10, # cv folds
metrics = metric_set(rmse, mae),
grid = penalty_grid # penalty grid defined above
)
# creation of cv folds
nba_cv10 <- vfold_cv(nba_clean, v = 10)
# fit & tune models
penalty_grid <- grid_regular(
penalty(range = c(-3, 1)), #log10 transformed
levels = 30)
tune_output <- tune_grid( # new function for tuning parameters
lasso_wf_tune, # workflow
resamples = nba_cv10, # cv folds
metrics = metric_set(rmse, mae),
grid = penalty_grid # penalty grid defined above
)
tune_output
autoplot(tune_output)
tune_output
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(tidymodels)
tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions
set.seed(123)
cars2018 <- read_csv("https://raw.githubusercontent.com/juliasilge/supervised-ML-case-studies-course/master/data/cars2018.csv")
# Cleaning
cars2018 <- cars2018 %>%
select(-model_index)
head(cars2018)
lm_spec <-
linear_reg() %>%
set_engine(engine = 'lm') %>%
set_mode('regression')
full_rec <- recipe(mpg ~ ., data = cars2018) %>%
update_role(model, new_role = 'ID') %>% # we want to keep the name of the car model but not as a predictor or outcome
step_nzv(all_predictors()) %>% # removes variables with the same value
step_normalize(all_numeric_predictors()) %>% # important standardization step for LASSO
step_dummy(all_nominal_predictors())  # creates indicator variables for categorical variables
full_lm_wf <- workflow() %>%
add_recipe(full_rec) %>%
add_model(lm_spec)
full_model <- fit(full_lm_wf, data = cars2018)
full_model %>% tidy()
cars2018_cv <- vfold_cv(cars2018, v = 10)
mod1_cv <- fit_resamples(full_lm_wf,
resamples = cars2018_cv,
metrics = metric_set(rmse, rsq, mae)
)
mod1_cv %>%
collect_metrics()
# Tune and fit a LASSO model to the data (with CV)
set.seed(74)
# Create CV folds
data_cv10 <- vfold_cv(cars2018, v = 10)
# Lasso Model Spec with tune
lm_lasso_spec_tune <-
linear_reg() %>%
set_args(mixture = 1, penalty = tune()) %>% ## mixture = 1 indicates Lasso
set_engine(engine = 'glmnet') %>% #note we are using a different engine
set_mode('regression')
# Workflow (Recipe + Model)
lasso_wf_tune <- workflow() %>%
add_recipe(full_rec) %>% # recipe defined above
add_model(lm_lasso_spec_tune)
# Tune Model (trying a variety of values of Lambda penalty)
penalty_grid <- grid_regular(
penalty(range = c(-3, 1)), #log10 transformed
levels = 30)
tune_output <- tune_grid( # new function for tuning parameters
lasso_wf_tune, # workflow
resamples = data_cv10, # cv folds
metrics = metric_set(rmse, mae),
grid = penalty_grid # penalty grid defined above
)
tune_output
# Visualize Model Evaluation Metrics from Tuning
autoplot(tune_output)
# recipes & workflows
full_rec <- recipe(AllNBA ~ MP + FGA + eFG. + TRB + AST + STL + BLK + TOV + PTS, data = nba_clean) %>%
step_nzv(all_predictors()) %>% # removes variables with the same value
step_normalize(all_numeric_predictors())
lasso_wf_tune <- workflow() %>%
add_recipe(full_rec) %>% # recipe defined above
add_model(lm_lasso_spec_tune)
# fit & tune models
penalty_grid <- grid_regular(
penalty(range = c(-3, 1)), #log10 transformed
levels = 30)
tune_output <- tune_grid( # new function for tuning parameters
lasso_wf_tune, # workflow
resamples = nba_cv10, # cv folds
metrics = metric_set(rmse, mae),
grid = penalty_grid # penalty grid defined above
)
tune_output
autoplot(tune_output)
# fit & tune models
penalty_grid <- grid_regular(
penalty(range = c(-3, 1)), #log10 transformed
levels = 30)
tune_output <- tune_grid( # new function for tuning parameters
lasso_wf_tune, # workflow
resamples = nba_cv10, # cv folds
metrics = metric_set(rmse, mae, rsq),
grid = penalty_grid # penalty grid defined above
)
tune_output
autoplot(tune_output)
# fit & tune models
penalty_grid <- grid_regular(
penalty(range = c(-5, 1)), #log10 transformed
levels = 50)
tune_output <- tune_grid( # new function for tuning parameters
lasso_wf_tune, # workflow
resamples = nba_cv10, # cv folds
metrics = metric_set(rmse, mae, rsq),
grid = penalty_grid # penalty grid defined above
)
tune_output
autoplot(tune_output)
best_penalty <- select_best(tune_output, metric = 'mae') # choose penalty value based on lowest cv mae
best_penalty
best_se_penalty <- select_by_one_std_err(tune_output, metric = 'mae', desc(penalty)) # choose largest penalty value within 1 se of the lowest cv mae
best_se_penalty
#  calculate/collect CV metrics
best_penalty <- select_best(tune_output, metric = 'mae')
best_penalty
# fit & tune models
penalty_grid <- grid_regular(
penalty(range = c(-5, 1)), #log10 transformed
levels = 50)
tune_output <- tune_grid( # new function for tuning parameters
lasso_wf_tune, # workflow
resamples = nba_cv10, # cv folds
metrics = metric_set(rmse, mae, rsq),
grid = penalty_grid # penalty grid defined above
)
tune_output
autoplot(tune_output)
#  calculate/collect CV metrics
best_penalty <- select_best(tune_output, metric = 'mae')
best_penalty
# fit & tune models
penalty_grid <- grid_regular(
penalty(range = c(-5, 1)), #log10 transformed
levels = 50)
nbatune_output <- tune_grid( # new function for tuning parameters
lasso_wf_tune, # workflow
resamples = nba_cv10, # cv folds
metrics = metric_set(rmse, mae, rsq),
grid = penalty_grid # penalty grid defined above
)
nbatune_output
autoplot(nbatune_output)
#  calculate/collect CV metrics
best_penalty <- select_best(nbatune_output, metric = 'mae')
best_penalty
# recipes & workflows
full_rec <- recipe(AllNBA ~ MP + FGA + eFG. + TRB + AST + STL + BLK + TOV + PTS, data = nba_clean) %>%
step_nzv(all_predictors()) %>% # removes variables with the same value
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors())
lasso_wf_tune <- workflow() %>%
add_recipe(full_rec) %>% # recipe defined above
add_model(lm_lasso_spec_tune)
# fit & tune models
penalty_grid <- grid_regular(
penalty(range = c(-5, 1)), #log10 transformed
levels = 50)
nbatune_output <- tune_grid( # new function for tuning parameters
lasso_wf_tune, # workflow
resamples = nba_cv10, # cv folds
metrics = metric_set(rmse, mae, rsq),
grid = penalty_grid # penalty grid defined above
)
nbatune_output
autoplot(nbatune_output)
# recipes & workflows
full_rec <- recipe(AllNBA ~ ., data = nba_clean) %>%
step_nzv(all_predictors()) %>% # removes variables with the same value
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors())
lasso_wf_tune <- workflow() %>%
add_recipe(full_rec) %>% # recipe defined above
add_model(lm_lasso_spec_tune)
# fit & tune models
penalty_grid <- grid_regular(
penalty(range = c(-5, 1)), #log10 transformed
levels = 50)
nbatune_output <- tune_grid( # new function for tuning parameters
lasso_wf_tune, # workflow
resamples = nba_cv10, # cv folds
metrics = metric_set(rmse, mae, rsq),
grid = penalty_grid # penalty grid defined above
)
nbatune_output
autoplot(nbatune_output)
# recipes & workflows
full_rec <- recipe(AllNBA ~ .-Player-Pos-A..-Tm, data = nba_clean) %>%
step_nzv(all_predictors()) %>% # removes variables with the same value
step_normalize(all_numeric_predictors())
# recipes & workflows
full_rec <- recipe(AllNBA ~ ., data = nba_clean) %>%
step_nzv(all_predictors()) %>% # removes variables with the same value
step_normalize(all_numeric_predictors())%>%
step_rm(Player)%>%
step_rm(Pos)%>%
step_rm(A..)%>%
step_rm(Tm)
lasso_wf_tune <- workflow() %>%
add_recipe(full_rec) %>% # recipe defined above
add_model(lm_lasso_spec_tune)
# fit & tune models
penalty_grid <- grid_regular(
penalty(range = c(-5, 1)), #log10 transformed
levels = 50)
nbatune_output <- tune_grid( # new function for tuning parameters
lasso_wf_tune, # workflow
resamples = nba_cv10, # cv folds
metrics = metric_set(rmse, mae, rsq),
grid = penalty_grid # penalty grid defined above
)
nbatune_output
autoplot(nbatune_output)
# recipes & workflows
full_rec <- recipe(AllNBA ~ ., data = nba_clean) %>%
step_nzv(all_predictors()) %>% # removes variables with the same value
step_normalize(all_numeric_predictors())%>%
step_rm(Player)%>%
step_rm(Pos)%>%
step_rm(A..)%>%
step_rm(Tm)%>%
step_rm(AllStar)
lasso_wf_tune <- workflow() %>%
add_recipe(full_rec) %>% # recipe defined above
add_model(lm_lasso_spec_tune)
# fit & tune models
penalty_grid <- grid_regular(
penalty(range = c(-5, 1)), #log10 transformed
levels = 50)
nbatune_output <- tune_grid( # new function for tuning parameters
lasso_wf_tune, # workflow
resamples = nba_cv10, # cv folds
metrics = metric_set(rmse, mae, rsq),
grid = penalty_grid # penalty grid defined above
)
nbatune_output
autoplot(nbatune_output)
# recipes & workflows
full_rec <- recipe(AllNBA ~ ., data = nba_clean) %>%
step_nzv(all_predictors()) %>% # removes variables with the same value
step_normalize(all_numeric_predictors())%>%
step_rm(Player)%>%
step_rm(Pos)%>%
step_rm(A...)%>%
step_rm(Tm)%>%
step_rm(AllStar)
lasso_wf_tune <- workflow() %>%
add_recipe(full_rec) %>% # recipe defined above
add_model(lm_lasso_spec_tune)
# fit & tune models
penalty_grid <- grid_regular(
penalty(range = c(-5, 1)), #log10 transformed
levels = 50)
nbatune_output <- tune_grid( # new function for tuning parameters
lasso_wf_tune, # workflow
resamples = nba_cv10, # cv folds
metrics = metric_set(rmse, mae, rsq),
grid = penalty_grid # penalty grid defined above
)
nbatune_output
autoplot(nbatune_output)
# library statements
# read in data
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(tidymodels)
tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions
set.seed(123)
nba2021pergame <- read.csv("2021-PerGame-Player-Stats.csv")
head(nba2021pergame)
View(nba2021pergame)
# recipes & workflows
full_rec <- recipe(AllNBA ~ ., data = nba_clean) %>%
step_nzv(all_predictors()) %>% # removes variables with the same value
step_normalize(all_numeric_predictors())
# data cleaning
nba_clean <-
nba2021pergame %>% distinct(Player, .keep_all = TRUE) %>%
select(-ï..Rk)
# creation of cv folds
nba_cv10 <- vfold_cv(nba_clean, v = 10)
# model spec
lm_spec <-
linear_reg() %>%
set_engine(engine = 'lm') %>%
set_mode('regression')
lm_lasso_spec_tune <-
linear_reg() %>%
set_args(mixture = 1, penalty = tune()) %>% ## mixture = 1 indicates Lasso
set_engine(engine = 'glmnet') %>% #note we are using a different engine
set_mode('regression')
knn_spec <-
nearest_neighbor() %>% # new type of model!
set_args(neighbors = tune()) %>% # tuning parameter is neighbor; tuning spec
set_engine(engine = 'kknn') %>% # new engine
set_mode('regression')
# recipes & workflows
full_rec <- recipe(AllNBA ~ ., data = nba_clean) %>%
step_nzv(all_predictors()) %>% # removes variables with the same value
step_normalize(all_numeric_predictors())
lasso_wf_tune <- workflow() %>%
add_recipe(full_rec) %>% # recipe defined above
add_model(lm_lasso_spec_tune)
knn_rec <- recipe( AllNBA ~ . , data = nba_clean) %>%
step_nzv(all_predictors()) %>%
step_novel(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors())
knn_wf <- workflow() %>%
add_model(knn_spec) %>%
add_recipe(knn_rec)
# fit & tune models
penalty_grid <- grid_regular(
penalty(range = c(-5, 1)), #log10 transformed
levels = 50)
nbatune_output <- tune_grid( # new function for tuning parameters
lasso_wf_tune, # workflow
resamples = nba_cv10, # cv folds
metrics = metric_set(rmse, mae, rsq),
grid = penalty_grid # penalty grid defined above
)
nbatune_output
autoplot(nbatune_output)
#  calculate/collect CV metrics
best_penalty <- select_best(nbatune_output, metric = 'mae')
# library statements
# read in data
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(tidymodels)
tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions
set.seed(123)
nba2021pergame <- read.csv("2021-PerGame-Player-Stats.csv")
head(nba2021pergame)
nba2021_2022salaries <- read.csv("2021-2022-Player-Salaries.csv")
View(nba2021_2022salaries)
