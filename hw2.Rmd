```{r hw2_setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)
```

# Homework 2 {-}

<center>
**Due Thursday, September 30 at 11:59pm CST on [Moodle](https://moodle.macalester.edu/mod/assign/view.php?id=27980)**
</center>

**Deliverables:** Please use [this template](template_rmds/hw2.Rmd) to knit an HTML document. Convert this HTML document to a PDF by opening the HTML document in your web browser. *Print* the document (Ctrl/Cmd-P) and change the destination to "Save as PDF". Submit this one PDF to Moodle.

Alternatively, you may knit your Rmd directly to PDF if you have LaTeX installed.



<br><br><br>




## Project Work {-}

### Instructions {-} 

**Goal:** Begin an analysis of your dataset to answer your **regression** research question.

<br>

**Collaboration:** Form a team (2-3 members) for the project and this part can be done as a team. Only one team member should submit a Project Work section. Make sure you include the full names of all of the members in your write up. 

<br>

**Data cleaning:** If your dataset requires any cleaning (e.g., merging datasets, creation of new variables), first consult the [R Resources page](r-resources.html) to see if your questions are answered there. If not, post on the #rcode-questions channel in our Slack workspace to ask for help. *Please ask for help early and regularly* to avoid stressful workloads.

<br>

### Required Analyses {-}

1. **Initial investigation: ignoring nonlinearity (for now)**
    a. Use ordinary least squares (OLS) by using the `lm` engine and LASSO (`glmnet` engine) to build  a series of initial regression models for your quantitative outcome as a function of the predictors of interest. (As part of data cleaning, exclude any variables that you don't want to consider as predictors.)
        - You'll need two model specifications, `lm_spec` and `lm_lasso_spec` (you'll need to tune this one).
    b. For each set of variables, you'll need a `recipe` with the `formula`, `data`, and pre-processing steps
        - You may want to have steps in your recipe that remove variables with near zero variance (`step_nzv()`), remove variables that are highly correlated with other variables (`step_corr()`), normalize all quantitative predictors (`step_normalize(all_numeric_predictors())`) and add indicator variables for any categorical variables (`step_dummy(all_nominal_predictors())`).
        - These models should not include any transformations to deal with nonlinearity. You'll explore this in the next investigation.
    c. Estimate the test performance of the models using CV. Report and interpret (with units) the CV metric estimates along with a measure of uncertainty in the estimate (`std_error` is readily available when you used `collect_metrics(summarize=TRUE)`).
        - Compare estimated test performance across the models. Which models(s) might you prefer?
    d. Use residual plots to evaluate whether some quantitative predictors might be better modeled with nonlinear relationships.
    e. Which variables do you think are the most important predictors of your quantitative outcome? Justify your answer. Do the methods you've applied reach consensus on which variables are most important? What insights are expected? Surprising?
        - Note that if some (but not all) of the indicator terms for a categorical predictor are selected in the final models, the whole predictor should be treated as selected.

<br>

#### Your Work {-}

a & b.

```{r}
# library statements 
# read in data
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(rpart.plot)
library(tidymodels) 
tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions
set.seed(123)

nba2021pergame <- read.csv("2021-PerGame-Player-Stats.csv")


nba2021_2022salaries <- read.csv("2021-2022-Player-Salaries.csv")
names(nba2021_2022salaries)[1] <- "Player"

nba2021pergame <- nba2021pergame %>% left_join(nba2021_2022salaries,by=c('Player')) %>%
  select(-AllStar)  %>%
  distinct(Player, .keep_all = TRUE) %>%
  filter(!is.na(Salary)) %>%
  na.omit()
head(nba2021pergame)
```

```{r}
# data cleaning


```

```{r}
# creation of cv folds

nba_cv10 <- vfold_cv(nba2021pergame, v = 10)

```

```{r}
# model spec
lm_spec <- 
    linear_reg() %>% 
    set_engine(engine = 'lm') %>% 
    set_mode('regression')
lm_lasso_spec_tune <- 
  linear_reg() %>%
  set_args(mixture = 1, penalty = tune()) %>% ## mixture = 1 indicates Lasso
  set_engine(engine = 'glmnet') %>% #note we are using a different engine
  set_mode('regression') 
knn_spec <- 
  nearest_neighbor() %>% # new type of model!
  set_args(neighbors = tune()) %>% # tuning parameter is neighbor; tuning spec
  set_engine(engine = 'kknn') %>% # new engine
  set_mode('regression')
tree_spec <- decision_tree() %>%
  set_engine(engine = 'rpart') %>%
  set_args(cost_complexity = tune(),  
           min_n = 2, 
           tree_depth = NULL) %>% 
  set_mode('classification') 
```

```{r}
# recipes & workflows
mod1 <- fit(lm_spec, 
    Salary ~ G+GS +FG +FGA + MP + PTS +STL + BLK + Age + ORB + DRB + AST + WS + X3P. + X2P., data = nba2021pergame)


full_rec <- recipe(Salary ~ G+ GS +FG +FGA + MP + PTS +STL + BLK + Age + TOV + PF + ORB + DRB + TRB + AST + WS + X3P + X3PA + X2P + X2PA + FT + FTA + FT., data = nba2021pergame) %>%
  step_nzv(all_predictors()) %>% # removes variables with the same value
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors())
lasso_wf_tune <- workflow() %>% 
  add_recipe(full_rec) %>% # recipe defined above
  add_model(lm_lasso_spec_tune) 
knn_rec <- recipe(Salary ~ . , data = nba2021pergame) %>%
    step_nzv(all_predictors()) %>% 
    step_novel(all_nominal_predictors()) %>% 
    step_normalize(all_numeric_predictors()) %>%  
    step_dummy(all_nominal_predictors())
knn_wf <- workflow() %>%
  add_model(knn_spec) %>% 
  add_recipe(knn_rec)
```

```{r}
# fit & tune models
penalty_grid <- grid_regular(
  penalty(range = c(3, 7)), #log10 transformed 
  levels = 10)

nbatune_output <- tune_grid( # new function for tuning parameters
  lasso_wf_tune, # workflow
  resamples = nba_cv10, # cv folds
  metrics = metric_set(mae),
  grid = penalty_grid # penalty grid defined above
)
nbatune_output
autoplot(nbatune_output)


mod1 %>% tidy()

```

c.

```{r}
#  calculate/collect CV metrics
best_penalty <- select_best(nbatune_output, metric = 'mae')
best_penalty
```

 
d.

```{r}
# visual residuals
mod1_output <- mod1 %>% 
    predict(new_data = nba2021pergame) %>%
    bind_cols(nba2021pergame) %>%
    mutate(resid = Salary - .pred)
ggplot(mod1_output, aes(x = .pred, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()

```

```{r} 
nba2021pergame <- nba2021pergame %>%
mutate(Pos = stringr::str_replace(stringr::str_sub(Pos,1,2),"-",""))

nba2021pergame <- nba2021pergame %>%
  mutate(Pos = as.factor(Pos))
  
data_fold <- vfold_cv(nba2021pergame, v = 10)

data_rec <- recipe(Pos ~ G+GS +FG +FGA + MP + PTS +STL + BLK + Age + ORB + DRB + AST + WS + X3P. + X2P., data = nba2021pergame)

data_wf_tune <- workflow() %>%
  add_model(tree_spec) %>%
  add_recipe(data_rec)

param_grid <- grid_regular(cost_complexity(range = c(-5, -1)), levels = 10) 

tune_res <- tune_grid(
  data_wf_tune, 
  resamples = data_fold, 
  grid = param_grid, 
  metrics = metric_set(accuracy) #change this for regression trees
)

autoplot(tune_res) + theme_classic()

best_complexity <- select_by_one_std_err(tune_res, metric = 'accuracy', desc(cost_complexity))
best_complexity %>%
  pull(cost_complexity)
data_wf_final <- finalize_workflow(data_wf_tune, best_complexity)

pos_final_fit <- fit(data_wf_final, data = nba2021pergame)


tune_res %>% 
  collect_metrics() %>%
  filter(cost_complexity == best_complexity %>% pull(cost_complexity))

tree_mod_lowcp <- fit(
    data_wf_tune %>%
        update_model(tree_spec %>% set_args(cost_complexity = .0001)),
    data = nba2021pergame
)
tree_mod_highcp <- fit(
    data_wf_tune %>%
        update_model(tree_spec %>% set_args(cost_complexity = .03)),
    data = nba2021pergame
)

# Plot all 3 trees in a row
#par(mfrow = c(1,3))
tree_mod_lowcp %>% extract_fit_engine() %>% rpart.plot()
pos_final_fit %>% extract_fit_engine() %>% rpart.plot()
tree_mod_highcp %>% extract_fit_engine() %>% rpart.plot()
```

```{r}
#Logistic

# Make sure you set reference level (to the outcome you are NOT interested in)
nba2021pergame <- nba2021pergame %>%
  mutate(nba2021pergame = relevel(factor(AllNBA), ref='0')) #set reference level

# Logistic Regression Model Spec
logistic_spec <- logistic_reg() %>%
    set_engine('glm') %>%
    set_mode('classification')

# Recipe
logistic_rec <- recipe(AllNBA ~ G+GS +FG +FGA + MP + PTS +STL + BLK + Age + ORB + DRB + AST + WS + X3P. + X2P., data = nba2021pergame)

# Workflow (Recipe + Model)
log_wf <- workflow() %>% 
    add_recipe(logistic_rec) %>%
    add_model(logistic_spec) 

# Fit Model
log_fit <- fit(log_wf, data = nba2021pergame)


```


```{r}
#LASSO Logistic

# Make sure you set reference level (to the outcome you are NOT interested in)
nba2021pergame <- nba2021pergame %>%
  mutate(nba2021pergame = relevel(factor(AllNBA), ref='0')) #set reference level

nba_cv10 <- vfold_cv(nba2021pergame, v = 10)


# Logistic LASSO Regression Model Spec
logistic_lasso_spec_tune <- logistic_reg() %>%
    set_engine('glmnet') %>%
    set_args(mixture = 1, penalty = tune()) %>%
    set_mode('classification')

# Recipe
logistic_rec <- recipe(AllNBA ~ G+GS +FG +FGA + MP + PTS +STL + BLK + Age + ORB + DRB + AST + WS + X3P. + X2P., data = nba2021pergame) %>%
    step_normalize(all_numeric_predictors()) %>% 
    step_dummy(all_nominal_predictors())

# Workflow (Recipe + Model)
log_lasso_wf <- workflow() %>% 
    add_recipe(logistic_rec) %>%
    add_model(logistic_lasso_spec_tune) 

# Tune Model (trying a variety of values of Lambda penalty)
penalty_grid <- grid_regular(
  penalty(range = c(2, 7)), #log10 transformed  (kept moving min down from 0)
  levels = 20)

tune_output <- tune_grid( 
  log_lasso_wf, # workflow
  resamples = nba_cv10, # cv folds
  metrics = metric_set(roc_auc,accuracy),
  control = control_resamples(save_pred = TRUE, event_level = 'second'),
  grid = penalty_grid # penalty grid defined above
)

# Visualize Model Evaluation Metrics from Tuning
autoplot(tune_output) + theme_classic()
```

```{r}
ggplot(nba2021pergame, aes(x=ORB,y=X3P.)) +
    geom_point()

nba_sub <- nba2021pergame %>%
    select(ORB, X3P.)

# Run k-means for k = centers = 3
set.seed(253)
kclust_k5 <- kmeans(nba_sub, centers = 5)

# Display the cluter assignments
kclust_k5$cluster

# Add a variable (kclust_3) to the original dataset 
# containing the cluster assignments
nba_clust <- nba2021pergame %>%
    mutate(kclust_5 = factor(kclust_k5$cluster))
ggplot(nba2021pergame, aes(x=ORB,y=X3P., colour = kclust_5)) +
    geom_point()
```

e.

<br>

2. **Summarize investigations**
    - Decide on an overall best model based on your investigations so far. To do this, make clear your analysis goals. Predictive accuracy? Interpretability? A combination of both?



<br>

3. **Societal impact**
    - Are there any harms that may come from your analyses and/or how the data were collected?
    -> There seems to be no harm in the way that this data was collected, as there is no experiment involved: basketball stats are gathered via game observation alone. Given that we are using our model to make predictions that may not necessarily come true, there is potential for our analyses to cause confusion or facilitate the spread of misinformation.
    - What cautions do you want to keep in mind when communicating your work?
    -> As mentioned above, it will be important to communicate that our findings are predictions based on previous data, and by no means should they be taken as fact. There are a variety of factors, such as injuries and off-court issues, that cannot be predicted but can have substantial impact on player performance.



<br><br><br>
